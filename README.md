# Thesis
Bert Method for German Language Sentiment Classification:

In the contemporary landscape of sentiment analysis, the Bidirectional Encoder Representations from Transformers (BERT) and Cross lingual Language Model Robustly Optimized BERT (XLM RoBERTa) models have emerged as powerful tools for understanding and classifying sentiments in text. This thesis delves into their test and application in the context of German language sentiment classification. Testing has been carried out for four distinct labeled datasets and application to an unlabeled dataset related to nuclear energy has been done. In coping with noisy input, specifically spelling mistakes by swapping the characters in the word using nlpaug library, a comprehensive comparative analysis is conducted, pitting BERT against XLM RoBERTa, across four labeled datasets. Notably, the datasets, characterized by varying text lengths, exhibit differing sensitivities to spelling errors. XLM RoBERTa demonstrates superior performance in two datasets, showcasing its adaptability to varied linguistic differences. Conversely, BERT outperforms XLMRoBERTa in one dataset, emphasizing the significance of considering specific dataset characteristics. Expanding the analysis to unlabeled dataset related to nuclear energy, XLM RoBERTa predominantly predicts neutral sentiments, whereas BERT includes negative sentiments also, though with almost negligible percentage of positive tweets in both models.

